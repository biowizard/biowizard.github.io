---
layout: mdpage
---

## Job Files

Jobs handled by Biowizard are expressed in 'job files'. These files are written in the YAML markup language, which is reasonably human-readable, and can be edited in a fairly straightforward way.

Below is an example job file which uses STAR to map a read file in fastq format to a genome index. 
This job retrieves the genome index and the read file from an S3 bucket using provided credentials, performs the mapping, transforms the result into a sorted and indexed bam file, and pushes the result back into S3, along with any log files generated by the job.

    ---
    job: star

    credentials:
        s3_access_key: MYAWSACCESSKEY
        s3_secret_key: MYAWSSECRETKEY

    inputs:
        genome: s3://my-s3-bucket/genome-indexes/mygenomeindex.tar.gz
        fastq:
            - file: s3://my-s3-bucket/test_100reads.fastq.gz

    outputs:
        sorted_bam: s3://my-s3-bucket/sorted.bam
        sorted_bam_index: s3://my-s3-bucket/sorted.bam.bai
        logs: s3://my-s3-bucket/logs

### Job

Biowizard is a system designed to run a number of common bioinformatics jobs. Each job file needs to include the type of job that should be run. In this example, we're instructing Biowizard to use STAR to map reads to a particular genome.

    job: star


### Credentials

Most Biowizard operations will require retrieving content from a remote source, possibly a web server, an FTP server, or an S3 bucket. Access to these resources usually requires a set of credentials such as a username / password, or an access key / secret key. These credentials are provided to Biowizard in the 'credentials' section of the job file.

    credentials:
        s3_access_key: MYAWSACCESSKEY
        s3_secret_key: MYAWSSECRETKEY

Sometimes user workflows will require access to multiple S3 buckets (with different keys). This can be achieved by nesting additional credentials within the job file.

    credentials:
        s3_access_key: MYAWSACCESSKEY
        s3_secret_key: MYAWSSECRETKEY
        some_other_bucket:
	        s3_access_key: ANOTHERAWSACCESSKEY
	        s3_secret_key: ANOTHERAWSSECRETKEY

### Inputs

The inputs section of a job file tells Biowizard the locations and roles of input files. In this example, we retrieve a file named "mygenomeindex.tar.gz" to represent an indexed genome, and a file called "test_100reads.fastq.gz" as a file containing that should be mapped. "genome" and "fastq" here describe the role of the input file in the job. Valid input file roles vary depending on the job being executed.

    inputs:
        genome: s3://my-s3-bucket/genome-indexes/mygenomeindex.tar.gz
        fastq:
            - file: s3://my-s3-bucket/test_100reads.fastq.gz



### Outputs

Where the inputs section of a job file tells Biowizard where to obtain input files, the outputs section describes the roles of any output files that should be stored to an external location after Biowizard has completed its job. Valid output file roles vary depending on the job being executed. In this example, a sorted BAM file, and the index for that sorted BAM file are copied to an S3 bucket.

    outputs:
        sorted_bam: s3://my-s3-bucket/sorted.bam
        sorted_bam_index: s3://my-s3-bucket/sorted.bam.bai
        logs: s3://my-s3-bucket/logs
